{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOjqE4+n1/JCqC3Rtk9s1kH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"T48qJd-141se","executionInfo":{"status":"ok","timestamp":1743318826416,"user_tz":-210,"elapsed":5832,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}},"outputId":"07d52ddf-af55-4328-e210-1aad61a640f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"]}],"source":["! pip install transformers"]},{"cell_type":"markdown","source":["# Behind Pipelines"],"metadata":{"id":"PGmNNivrKKAQ"}},{"cell_type":"markdown","source":["## pipeline usage"],"metadata":{"id":"nkY6p5DeKRb6"}},{"cell_type":"code","source":["# Pipelines\n","from transformers import pipeline\n","raw_inputs =  [\n","        \"I've been waiting for a HuggingFace course my whole life.\",\n","        \"I hate this so much!\",\n","        ]\n","# Official way:\n","# model = pipeline(\"Taks_Name\", possible_params)\n","# model(input, possible_params)\n","model = pipeline(\"sentiment-analysis\")\n","model(raw_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"YpGDCdsD5Dt3","executionInfo":{"status":"ok","timestamp":1743325601482,"user_tz":-210,"elapsed":542,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}},"outputId":"0a207880-f5ad-4328-ffe0-d171350a569c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Device set to use cpu\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n"," {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## AutoModel and AutoTokenizer\n","The **output of AutoModel** contians last_hidden_state which shape is made of\n","1.    the input sequnce length,\n","2.    batch size (number of inputs), and\n","3.    hidden state size (the vector dimention of each input)."],"metadata":{"id":"nHjEzXS1KVn1"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"    # A model name from the list of HuggingFace models.\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","raw_inputs = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","]\n","inputs    = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)\n","\n","\n","from transformers import AutoModel\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"    # NOTE: This model should be exactly the same model used for the AutoTokenizer\n","model      = AutoModel.from_pretrained(checkpoint)\n","output    = model(**inputs)\n","\n","print(output.keys())\n","print (output.last_hidden_state.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ov47vc4xIXNX","executionInfo":{"status":"ok","timestamp":1743325606842,"user_tz":-210,"elapsed":3,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}},"outputId":"47243850-5446-4128-f9e6-a6ed8a778ab7"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n","          2607,  2026,  2878,  2166,  1012,   102],\n","        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n","odict_keys(['last_hidden_state'])\n","torch.Size([2, 16, 768])\n"]}]},{"cell_type":"markdown","source":["## Model\n","\n","The **model** is represented by its embeddings layer and the subsequent layers.\n","*   The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token.\n","*   The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.\n"],"metadata":{"id":"0SZL_ey5M7IY"}},{"cell_type":"code","source":["from transformers import BertConfig, BertModel\n","config = BertConfig()\n","#print (config)\n","model = BertModel(config)                              # Randomly initialized model\n","model = BertModel.from_pretrained(\"bert-base-cased\")  # initialize with the bert-based-cased pretrained model\n","\n","input_seq = [\"Hello!\", \"Cool.\", \"Nice!\"]\n","encoded_seq = [\n","    [101, 7592, 999, 102],\n","    [101, 4658, 1012, 102],\n","    [101, 3835, 999, 102],\n","]\n","model_input = torch.tensor(encoded_seq)\n","output = model(model_input)\n","print(output.keys())\n","model.save_pretrained(\"Directory_name_path\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EiGdjfD8_5M2","executionInfo":{"status":"ok","timestamp":1743325778521,"user_tz":-210,"elapsed":5669,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}},"outputId":"6a31b396-27c1-43e9-cd95-3ea45594078f"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["odict_keys(['last_hidden_state', 'pooler_output'])\n"]}]},{"cell_type":"markdown","source":["## Model Head\n","\n","**AutoModelForSequenceClassification** contains AutoModel + Head. When AutoModel delivers [seq_length, betch_size, hidden_state], this model process the output through one more step and delivers \"logits\".\n","\n","The following **archs** handle the Model-Head for variouse tasks:\n"," *   *Model (retrieve the hidden states)\n"," *   *ForCausalLM\n"," *   *ForMaskedLM\n"," *   *ForMultipleChoice\n"," *   *ForQuestionAnswering\n"," *   *ForSequenceClassification\n"," *   *ForTokenClassification\n"," *   ..."],"metadata":{"id":"1UeZeAqsKU3x"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","classifier = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","output = classifier(**inputs)\n","print(output.keys())\n","print(output.logits.shape)\n","\n","import torch\n","prediction = torch.nn.functional.softmax(output.logits, dim=-1)\n","print (prediction)\n","print(classifier.config.id2label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kgrejyzlCyGO","executionInfo":{"status":"ok","timestamp":1743325872819,"user_tz":-210,"elapsed":558,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}},"outputId":"dc0bc3e3-82c1-447e-d151-a5eac39ce343"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["odict_keys(['logits'])\n","torch.Size([2, 2])\n","tensor([[0.4370, 0.5630],\n","        [0.4432, 0.5568]], grad_fn=<SoftmaxBackward0>)\n","{0: 'LABEL_0', 1: 'LABEL_1'}\n"]}]},{"cell_type":"markdown","source":["## Tokenizer\n","Tokenizatin Algorithms are:\n","1. word-based\n","2. character-based\n","3. subword-based (BPE, WordPiece , unigram(or sentencePiece))\n","\n","Tokenizer processes sequences in two steps\n","1. tokenize()\n","2. convert_tokens_to_ids().\n","\n","You could use these steps (Aproach 1 in the following code) or simply call the the tokenizer object itself (Aproach 2).\n","\n","IDs could be decodeed into sequences using decode().\n","\n","**Note:** sometimes the ids created by Aproach1 are not the same as ids created by tokenize.__call__ (Approach2).\n","Infact, some models add special words at the beginning or at the end (or both!) of the sequences.\n","\n","In any case, the tokenizer.__call__ knows which ones are expected and will deal with this for you, while tokenize+convert_tokens_to_ids returns exactly the same ids related to the sequence tokens."],"metadata":{"id":"-nu-VdhTNQRf"}},{"cell_type":"code","source":["# Saving tokens:\n","from transformers import BertTokenizer\n","checkpoint = 'bert-base-cased'\n","tokenizer = BertTokenizer.from_pretrained(checkpoint)\n","tokenizer.save_pretrained(\"a_dirctory_to_save_tokenizer\")\n","\n","# How tokenizer works:\n","import torch\n","from transformers import AutoModelForSequenceClassification\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","# Aprotch 1: Internal Steps\n","from transformers import  AutoTokenizer\n","the_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","raw_input  = \"This is my input data!\"\n","tokens     = the_tokenizer.tokenize(raw_input)\n","IDs        = the_tokenizer.convert_tokens_to_ids(tokens)\n","IDs_tensor = torch.tensor([IDs])            # This tensor of ids could be fed into a model. the [] is addede to handle the model input compatibility\n","decoded_string = the_tokenizer.decode(IDs)  # converts IDs to tokens and group them together inorder to create a string\n","\n","\n","output = model(IDs_tensor)\n","print(output.logits)\n","\n","# Aprotch 2: use the tokenizer object\n","# simply use the object of AutoTokenizer (or any other Tokenizer such as BertTokenizer Object)\n","from transformers import  AutoTokenizer\n","input = the_tokenizer(raw_input, padding=True, truncation = True , return_tensors='pt')\n","output = model(**input)\n","print(output.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pAfXysE795de","executionInfo":{"status":"ok","timestamp":1743325938071,"user_tz":-210,"elapsed":1110,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}},"outputId":"19068167-670a-4085-f118-cd95f8a7ffbc"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[-0.0036, -0.0829]], grad_fn=<AddmmBackward0>)\n","tensor([[-0.1554,  0.0003]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["## Multiple Sequence\n","1. Batchig same length sequnces\n","2. Batchig different length sequnces\n","    1. Padding\n","    2. Attention Mask\n","3. Truncate the sequnce if they are too long. Most models can handle sequences of up to 512 or 1024 tokens.\n","    1.  Longformer and LED  are models that support very long sequences.\n","    2. to truncate a sequence: tr_seq = seq[:max], while max is the maximum length of a sequence which could be supported by the model\n","\n","**Note**: the padding token-id could be found in **tokenizer.pad_token_id**\n"],"metadata":{"id":"OnIGBmT1QHtd"}},{"cell_type":"code","source":["# 1) Batchig same length sequnces\n","batched_ids = [IDs,IDs]\n","bectched_output = model(torch.tensor(batched_ids))\n","\n","# 2) Batchig different length sequnces\n","seq1 = [200, 200, 200]\n","seq2 = [200, 200]\n","batched_ids = [seq1, seq2]\n","#print(model(torch.tensor(batched_ids)).logits)  # Got Error. Need Padding and Mask\n","\n","# 2-1) Padding\n","padding_ids = tokenizer.pad_token_id\n","padded_batched_ids = [seq1, [200, 200, padding_ids]]\n","print(model(torch.tensor([seq1])).logits)\n","print(model(torch.tensor([seq2])).logits)\n","print(model(torch.tensor(padded_batched_ids)).logits) # The result are not the same! Need to define whic value should be considered in the calculations. To do so, we use Attenstion Mask\n","\n","# 2-2) Attention Mask\n","attention_mask = [\n","    [1, 1, 1],\n","    [1, 1, 0],\n","]   # 0 Shows the related valuse shoul not be considered in the calculations\n","\n","print(model(\n","    torch.tensor(padded_batched_ids),\n","    attention_mask = torch.tensor(attention_mask)\n","    ).logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iw1LHp-8UUf4","executionInfo":{"status":"ok","timestamp":1743326049857,"user_tz":-210,"elapsed":740,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}},"outputId":"210f12af-5661-4f3c-fc96-451fa72ecb4e"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.2260,  0.0486]], grad_fn=<AddmmBackward0>)\n","tensor([[-0.2355,  0.0344]], grad_fn=<AddmmBackward0>)\n","tensor([[-0.2260,  0.0486],\n","        [-0.1968, -0.0216]], grad_fn=<AddmmBackward0>)\n","tensor([[-0.2260,  0.0486],\n","        [-0.2355,  0.0344]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["## Tokenized or Multiple sequences\n","Tokenizer Object can handle Padding, Mask Attention, as well as Truncate automatically.\n","\n","padding could be done according to:\n","* The  Longest sequnce (\"longest\")\n","* The model maximum supported length (\"max_length\")\n","* The specified max length (padding=\"max_length\",max_length=8\")\n","\n","Truncation is supported in the following forms:\n","* Truncate sequences that are longer than the model max length using truncation=True . For example BERT or DistilBERT max-length are 512\n","*  Truncate the sequences that are longer than the specified max length using (truncation=True, max_length=8)\n","\n","Tokenizer could retunrs tensor in the following formats:\n","*  PyTorch tensors     (return_tensors=\"pt\")\n","*  TensorFlow tensors  (return_tensors=\"tf\")\n","*  Numpy tensors       (return_tensors=\"np\"\n","\n","Lets take a look:"],"metadata":{"id":"kwL4MV8URgxr"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","input_sequences = [\"This is the first sequence of tokens\", \"And this is the second one\", \"The list could be continued !\"]\n","input_sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer  = AutoTokenizer.from_pretrained (checkpoint)\n","\n","input = tokenizer(input_sequences, padding = \"longest\")                      # padding could be Longest sequnce\n","input = tokenizer(input_sequences, padding = \"max_length\")                   # Or The model maximum supported length\n","input = tokenizer(input_sequences, padding = \"max_length\", max_length=8)     # Or The specified max length (padding=\"max_length\",max_length=8\")\n","\n","inputs = tokenizer(input_sequences, truncation=True)                         # Will truncate the sequences that are longer than the model max length(512 for BERT or DistilBERT)\n","inputs = tokenizer(input_sequences, truncation=True, max_length=8)           # Will truncate the sequences that are longer than the specified max length\n","\n","inputs = tokenizer(input_sequences, padding=True, return_tensors=\"pt\")       # Returns PyTorch tensors\n","inputs = tokenizer(input_sequences, padding=True, return_tensors=\"tf\")       # Returns TensorFlow tensors\n","inputs = tokenizer(input_sequences, padding=True, return_tensors=\"np\")       # Returns Numpy tensors"],"metadata":{"id":"vHaD2VNaRojz","executionInfo":{"status":"ok","timestamp":1743326079881,"user_tz":-210,"elapsed":567,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["## Altogether"],"metadata":{"id":"JUMHowKiQz4z"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","input_sequences = [\"This is the first sequence of tokens\", \"And this is the second one\", \"The list could be continued !\"]\n","input_sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model      = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","tokenizer  = AutoTokenizer.from_pretrained (checkpoint)\n","inputs = tokenizer(input_sequences, padding=True, truncation=True, return_tensors=\"pt\")\n","output = model(**inputs)\n","print(output.keys(), output.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RqF-QlIPbtPk","executionInfo":{"status":"ok","timestamp":1743326143384,"user_tz":-210,"elapsed":699,"user":{"displayName":"Raha Asemani","userId":"04585468247326135737"}},"outputId":"b77dd57e-9c0d-4829-c36f-0ad0ea77f732"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["odict_keys(['logits']) tensor([[-1.5607,  1.6123],\n","        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)\n"]}]}]}